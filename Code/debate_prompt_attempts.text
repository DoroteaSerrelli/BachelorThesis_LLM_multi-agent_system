###DEBATE AS ANSWER MULTIPLE CHOICE QUESTIONS

from metrics import extract_time_complexity
from utility_function import get_formatted_code_solution
from response_JSON_schema import schema_feedback


def get_multiple_choice_numbers_prompt(answers, readability_complexity, AGENTS_NO):
    deb_prompt = (
            'Here are some proposed solutions (0, 1, 2, ...) for the code generation problem:\n'
    )

    for i in range(0, AGENTS_NO):
        if (answers[i] != ""):
            deb_prompt += (f"\n**{i}.**\n---\n{answers[i]}\n"
                           f"* Time complexity: {extract_time_complexity(answers[i])}\n"
                           f"* Cognitive complexity: {readability_complexity[i]}\n"
                           f"---\n"
                           )

    deb_prompt += (f"""\nConsidering both time complexity and cognitive complexity, which of the proposed solutions is the best?
                            Respond with **only** the corresponding number of the best solution.
                            Your response must be a **single integer** between 0 and {AGENTS_NO - 1} with **no explanation**, **no text**, and **no punctuation**.
                            Responding with anything other than a number will be considered an error."""
                       )

    return deb_prompt
#----------------------------------------------------------------------------







def getDiscussionGivenAnswersFeedbackPrompt_NoComparing(answers, user_prompt, readability_complexity, AGENTS_NO):
    deb_prompt = (
        f'''Here are several solutions to the following code generation problem:
        -----
        CODE GENERATION TASK:
        {user_prompt}
        -----\n'''
        ' Each solution is identified by a unique number and includes:'
        '* The code;'
        '* a time complexity (in Big-O notation); '
        '* a cognitive complexity score (how difficult the solution is to understand and maintain). '
        'Your task is to select the best solution based on both time and cognitive complexity.'
    )

    for i in range(0, AGENTS_NO):
        if (answers[i] != ""):
            deb_prompt += (f"\n**{i}.**\n---\n{answers[i]}\n"
                           f"* Time complexity: {extract_time_complexity(answers[i])}\n"
                           f"* Cognitive complexity: {readability_complexity[i]}\n"
                           f"---\n"
                           )

    deb_prompt += (f"""\nOnce selected the best solution, answers with **only** the number of the chosen solution.
                        Your response must be a **single integer**, **without any explanations**, **no additional text**, and **no punctuation**.
                        Responding with anything other than a number will be considered an error.""")

    return deb_prompt




#--------------------------------------------------
# STRATEGIE DI DIBATTITO PROVATE

'''
    Extended version of the debate simulation.

    Features:
    - Uses feedback generation *without direct comparison* between solutions.
    - Introduces convergence check after 2 non-agreement rounds:
        - If all candidate solutions are equivalent in time and cognitive complexity,
          randomly selects one.
        - Otherwise, triggers self-refinement and continues debate.

    Used when comparison-based evaluation may bias or slow convergence.
'''


def simulate_complete_round(user_prompt, few_shot_prompt, agents, max_rounds=MAXROUNDS_NO):
    # === Phase 1: Each model independently generates a first response ===
    response = []  # Stores responses from all agents
    for i in range(AGENTS_NO):
        response.append(get_programmer_first_response(agents[i], few_shot_prompt, user_prompt))

    # Display all initial responses
    for i in range(AGENTS_NO):
        print(f"Response model {i}: {response[i]}")

    # === Phase 2: Iterative debate rounds ===
    current_round = 1
    not_agreement_rounds = 0  # numero di round in cui non ci è stata convergenza
    while current_round <= max_rounds:
        # === Measure readability (cognitive complexity) of each response ===
        readability_complexity = []  # Stores total cognitive complexity for each response
        details_readability_complexity = []  # Stores node-level breakdown of complexity
        import json

        for i in range(AGENTS_NO):
            response_json = json.loads(response[i])  # Parse JSON response
            total, details = get_cognitive_complexity(response_json["code"])
            print(tabulate(details, headers=["Complexity", "Node"], tablefmt="fancy_grid"))
            readability_complexity.append(total)
            details_readability_complexity.append(details)

        # === Construct the prompt for each agent to analyze others' responses ===
        debate_prompt = getDiscussionGivenAnswersFeedbackPrompt_NoComparing(response, user_prompt,
                                                                            readability_complexity,
                                                                            AGENTS_NO)  # prova per non comparazione

        # === Collect feedback from each agent (which solution they prefer) ===
        feedback = []
        for i in range(AGENTS_NO):
            feedback.append(get_feedback_value(get_refined_agreement(agents[i], debate_prompt)))

        print(f"\nRound {current_round} - Feedback")
        for i in range(AGENTS_NO):
            print(f"Feedback model {i}: {feedback[i]}\n")

        # === Check if all agents agree on the same solution ===
        possible_solutions = set(feedback)  # Unique solutions selected by the agents

        if len(possible_solutions) == 1:
            print("Agreement")
            print("\nFinal answer:")
            solution = str()
            for var in possible_solutions:
                solution = response[int(var)]
                print(solution)
            return solution  # Return the agreed-upon solution

        not_agreement_rounds += 1
        # Print all proposed candidate solutions for transparency
        for var in feedback:
            print("Candidate solution: " + str(var))

        k_responses = []

        if not_agreement_rounds >= 2:
            # Verifico se tutte le soluzioni hanno uguale complessità di tempo e uguale complessità di leggibilità
            k_responses = get_k_responses(response, feedback)
            if equals_time_complexity(k_responses) and equals_cognitive_complexity(readability_complexity):
                solution = str(get_random_element(k_responses))
                print("Agreement between equivalent solution")
                print("\nFinal answer:")

                print(solution)
                return solution  # Return the agreed-upon solution
            else:  # oppure inserire un modello mediatore che estrae la migliore soluzione
                response = do_self_refinement(agents, response, user_prompt)
                not_agreement_rounds = 0

        k_response = continue_debate_on_k_solutions(k_responses, user_prompt, response, readability_complexity, agents)

        if len(k_response) == 1:
            print("Agreement")
            print("\nFinal answer:")
            solution = str()
            for var in k_response:
                solution = response[int(var)]
                print(solution)
            return solution  # Return the agreed-upon solution

        """# === If disagreement persists, proceed with self-refinement strategy ===
        if len(k_response) == len(possible_solutions):
            response = debate_self_refinement(agents, response, user_prompt)"""

        not_agreement_rounds += 1
        round += 1

    # === If max rounds exceeded, apply majority voting to resolve ===
    vote_index = majority_voting(feedback)
    return response[vote_index]



def debate_with_self_refinement(user_prompt, few_shot_prompt, agents, max_rounds=MAXROUNDS_NO):
    """
        Simulates a multi-round debate among agents until consensus is reached or a fallback is triggered.

        Steps:
        1. Agents independently generate an initial response to a prompt.
        2. Each response is evaluated for readability using cognitive complexity metrics.
        3. Each agent reviews peer responses and provides feedback (votes for the best one).
        4. If no consensus is reached, agents refine their responses in subsequent rounds.
        5. After max rounds, majority voting is used to select the final answer.
    """

    # === Phase 1: Each model independently generates a first response ===

    response = []  # Stores responses from all agents
    for i in range(AGENTS_NO):
        response.append(get_programmer_first_response(agents[i], few_shot_prompt, user_prompt))

    # Display all initial responses
    for i in range(AGENTS_NO):
        print(f"Response model {i}: {response[i]}")

    # === Phase 2: Iterative debate rounds ===
    current_round = 1
    feedback = []
    feedback_allowed = []

    while current_round <= max_rounds:
        feedback_allowed.clear()

        # === Measure readability (cognitive complexity) of each response ===
        readability_complexity = []  # Stores total cognitive complexity for each response
        details_readability_complexity = []  # Stores node-level breakdown of complexity

        for i in range(AGENTS_NO):
            response_json = json.loads(response[i])  # Parse JSON response
            total, details = get_cognitive_complexity(response_json["code"])
            print(tabulate(details, headers=["Complexity", "Node"], tablefmt="fancy_grid"))
            readability_complexity.append(total)
            details_readability_complexity.append(details)

        # === Construct the prompt for each agent to analyze others' responses ===

        # Rimuovere le risposte con cognitive complexity pari a -1: errori sintattici

        counter = 0

        responses_allowed = {}  # le risposte che hanno cognitive complexity != -1
        readability_complexity_allowed = {} # i valori di cognitive complexity delle risposte che hanno cognitive complexity != -1

        for i in range(0, AGENTS_NO):
            if readability_complexity[i] != -1:
                counter += 1
                responses_allowed[i] = response[i]
                readability_complexity_allowed[i] = readability_complexity[i]

        # Formattazione per prompt di dibattito
        formatted_responses = get_formatted_responses(responses_allowed, readability_complexity_allowed, counter)
        debate_prompt = get_refined_debate_prompt(counter, user_prompt, formatted_responses)

        # === Collect feedback from each agent (which solution they prefer) ===

        for i in range(AGENTS_NO):
            feedback.append(get_feedback_value(get_refined_agreement(agents[i], debate_prompt)))

        print(f"\nRound {current_round} - Feedback")
        for i in range(0, AGENTS_NO):
            print(f"Feedback model {i}: {feedback[i]}\n")

        # Verify if feedback is an integer between 0 and AGENTS_NO-1

        for var in feedback:
            if 0 <= var < AGENTS_NO:
                feedback_allowed.append(var)


        # === Check if all agents agree on the same solution ===
        possible_solutions = set(feedback_allowed)  # Unique solutions selected by the agents

        if len(possible_solutions) == 0: # Tutte le risposte generate dai modelli sono sintatticamente errate
            return "-1" # Dibattito fallito

        if len(possible_solutions) == 1:
            print("Agreement")
            print("\nFinal answer:")
            solution = str()
            for var in possible_solutions:
                solution = response[int(var)]
                print(solution)
            return solution  # Return the agreed-upon solution


        # Print all proposed candidate solutions for transparency
        for var in feedback_allowed:
            print("Candidate solution: " + str(var))

        # === If disagreement persists, proceed with self-refinement strategy ===
        response = do_self_refinement(agents, response, readability_complexity, user_prompt)

        feedback.clear()

        current_round += 1

    # === If max rounds exceeded, apply majority voting to resolve ===
    vote_index = majority_voting(feedback_allowed)
    return response[int(vote_index)]


def debate_with_k_candidates(user_prompt, few_shot_prompt, agents, max_rounds=MAXROUNDS_NO):
    """
        Same as simulate_round(), but adds a strategy where agents debate over a smaller subset (k)
        of top candidate solutions when a deadlock occurs.

        - Promotes convergence by narrowing focus to fewer promising answers.
        - Self-refinement still applies if disagreement persists.
    """

    # === Phase 1: Each model independently generates a first response ===
    response = []  # Stores responses from all agents
    for i in range(AGENTS_NO):
        response.append(get_programmer_first_response(agents[i], few_shot_prompt, user_prompt))

    # Display all initial responses
    for i in range(AGENTS_NO):
        print(f"Response model {i}: {response[i]}")

    # === Phase 2: Iterative debate rounds ===
    current_round = 1
    feedback = []
    feedback_allowed = []

    while current_round <= max_rounds:
        feedback_allowed.clear()
        # === Measure readability (cognitive complexity) of each response ===
        readability_complexity = []  # Stores total cognitive complexity for each response
        details_readability_complexity = []  # Stores node-level breakdown of complexity
        import json

        for i in range(AGENTS_NO):
            response_json = json.loads(response[i])  # Parse JSON response
            total, details = get_cognitive_complexity(response_json["code"])
            print(tabulate(details, headers=["Complexity", "Node"], tablefmt="fancy_grid"))
            readability_complexity.append(total)
            details_readability_complexity.append(details)

        # === Construct the prompt for each agent to analyze others' responses ===
        debate_prompt = ""

        for i in range(AGENTS_NO):
            formatted_responses = get_formatted_responses(response, readability_complexity, AGENTS_NO)
            debate_prompt = get_refined_debate_prompt(AGENTS_NO, user_prompt, formatted_responses)
            print(f"Prompt token count for round {current_round}: {count_tokens(debate_prompt)}")

        for i in range(AGENTS_NO):
            feedback.append(get_feedback_value(get_refined_agreement(agents[i], debate_prompt)))

        print(f"\nRound {current_round} - Feedback")
        for i in range(0, AGENTS_NO):
            print(f"Feedback model {i}: {feedback[i]}\n")

        # Verify if feedback is an integer between 0 and AGENTS_NO-1

        for var in feedback:
            if 0 <= var < AGENTS_NO:
                feedback_allowed.append(var)

        # === Check if all agents agree on the same solution ===
        possible_solutions = set(feedback_allowed)  # Unique solutions selected by the agents

        if len(possible_solutions) == 1:
            print("Agreement")
            print("\nFinal answer:")
            solution = str()
            for var in possible_solutions:
                solution = response[int(var)]
                print(solution)
            return solution  # Return the agreed-upon solution

        if len(possible_solutions) == 0:
            for i in range(AGENTS_NO):
                response.append(get_programmer_first_response(agents[i], few_shot_prompt, user_prompt))

            # Display all initial responses
            for i in range(AGENTS_NO):
                print(f"Response model {i}: {response[i]}")
                current_round += 1
                continue

        # Print all proposed candidate solutions for transparency
        for var in feedback_allowed:
            print("Candidate solution: " + str(var))

        k_response = continue_debate_on_k_solutions(feedback_allowed, user_prompt, response, readability_complexity, agents)

        if len(k_response) == 1:
            print("Agreement")
            print("\nFinal answer:")
            solution = str()
            for var in k_response:
                solution = response[int(var)]
                print(solution)
            return solution  # Return the agreed-upon solution

        # === If disagreement persists, proceed with self-refinement strategy ===
        if len(k_response) == len(possible_solutions):
            response = do_self_refinement(agents, response, user_prompt)

        feedback.clear()

        current_round += 2

    # === If max rounds exceeded, apply majority voting to resolve ===
    vote_index = majority_voting(feedback_allowed)
    return response[int(vote_index)]


# === Debate over k candidate solutions ===
def continue_debate_on_k_solutions(k_candidates_chosen, user_prompt, responses, readability_complexity, agents):
    """
        Reduces the debate scope to a smaller set of k candidate solutions.

        Each agent analyzes and votes on the best among the reduced solution set.
        Used to resolve deadlock or improve convergence.
    """

    k_cognitive_complexity = {}

    for i in range(AGENTS_NO):
        # Temporarily replace agent responses with the k selected ones
        responses[i] = responses[int(k_candidates_chosen[i])]
        if i not in k_cognitive_complexity:  # keep track cognitive complexity of k solutions
            k_cognitive_complexity[i] = readability_complexity[i]

    # Remove duplicates
    responses = remove_duplicates(responses)
    k_cognitive_complexity = remove_duplicates(k_cognitive_complexity)

    # Build the prompt to analyze k-candidate solutions
    formatted_responses = get_formatted_responses(responses, k_cognitive_complexity, len(responses)) #AGENTS_NO = len(responses) perché considero k_candidates
    debate_prompt = get_refined_debate_prompt(AGENTS_NO, user_prompt, formatted_responses)
    print(f"Prompt token count on continue_debate_on_k_solutions: {count_tokens(debate_prompt)}")

    new_response = []

    for i in range(AGENTS_NO):
        # Agent generates a new opinion
        new_response.append(get_response(agents[i], user_prompt, debate_prompt))
        print(f"Improved model {i} response: {new_response[i]}")


    return new_response