'''
    Utility functions to manage evaluator agent to score a solution.
'''

import lmstudio as lms

from response_JSON_schema import evaluation_schema
from evaluation_prompt import instruct_prompt

# Weights in percentage (total = 100)
CORRECTNESS_WEIGHT = 40
SECURITY_WEIGHT = 10
MAINTAINABILITY_WEIGHT = 10
RELIABILITY_WEIGHT = 10
COMPILE_ERR_WEIGHT = 15
EXEC_ERR_WEIGHT = 15

# Maximum number of errors before score reaches 0
MAX_ERRORS = 10


def get_evaluator(type_model, temperature=1.0):
    """
    Initializes and returns an LLM evaluator instance based on the given model type.

    Args:
        type_model (str): The identifier of the model to use for evaluation.

    Returns:
        evaluator: An instance of the language model ready to evaluate code.
    """

    evaluator = lms.llm(type_model, config={"temperature": temperature})

    return evaluator


def eval_code(user_prompt, ai_response, evaluator):
    """
    Sends a prompt and a generated code response to the evaluator model for assessment.

    Args:
        user_prompt (str): The original task description provided by the user.
        ai_response (str): The code generated by the LLM agent.
        evaluator: The evaluator model instance.

    Returns:
        str: A structured JSON response containing evaluation scores and explanation.
    """
    prompt = instruct_prompt
    prompt = prompt.replace("{user_prompt}", user_prompt)
    prompt = prompt.replace("{ai_response}", ai_response)

    messages = [{"role": "user", "content": prompt}]
    response = evaluator.respond({"messages": messages}, response_format=evaluation_schema)
    return response.content


def extract_explanation(json_evaluation):
    """
    Extracts the textual explanation section from the evaluator's JSON response.

    Args:
        json_evaluation (str): The full JSON evaluation string returned by the evaluator.

    Returns:
        str: The explanation of the code evaluation, detailing errors or feedback.
    """
    import json

    str_json = json.loads(json_evaluation)
    explanation = str_json["Explanation"]

    return explanation


def extract_criteria_scores(json_evaluation):
    """
    Parses the evaluator's JSON response to extract individual evaluation scores.

    Args:
        json_evaluation (str): The full JSON evaluation string returned by the evaluator.

    Returns:
        dict: A dictionary containing numerical scores for all evaluation criteria.
    """
    import json
    str_json = json.loads(json_evaluation)

    criteria_scores = {
        "Correctness": int(str_json["Correctness"]),
        "Security": int(str_json["Security"]),
        "Maintainability": int(str_json["Maintainability"]),
        "Reliability": int(str_json["Reliability"]),
        "Compilation Errors": int(str_json["Compilation Errors"]),
        "Execution Errors": int(str_json["Execution Errors"])
    }

    return criteria_scores


def calculate_score_code(criteria_scores: dict):
    """
    Calculates a weighted average score based on individual evaluation criteria and penalties for errors.

    Args:
        criteria_scores (dict): A dictionary with raw scores and error counts from the evaluation.

    Returns:
        float: The final aggregated quality score of the code (range 0–100).
    """
    correctness = criteria_scores["Correctness"]
    security = criteria_scores["Security"]
    maintainability = criteria_scores["Maintainability"]
    reliability = criteria_scores["Reliability"]
    compilation_err = criteria_scores["Compilation Errors"]
    execution_err = criteria_scores["Execution Errors"]

    # Normalize error counts into 0–100 scores
    compilation_score = max(0, 100 - (100 * min(compilation_err, MAX_ERRORS) / MAX_ERRORS))
    execution_score = max(0, 100 - (100 * min(execution_err, MAX_ERRORS) / MAX_ERRORS))

    # Calculate final score as a weighted average
    weighted_avg = (
        CORRECTNESS_WEIGHT * correctness +
        SECURITY_WEIGHT * security +
        MAINTAINABILITY_WEIGHT * maintainability +
        RELIABILITY_WEIGHT * reliability +
        COMPILE_ERR_WEIGHT * compilation_score +
        EXEC_ERR_WEIGHT * execution_score
    ) / 100

    return weighted_avg
